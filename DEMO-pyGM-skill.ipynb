{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill estimation using graphical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use a simple model to predict a hidden \"skill level\" of players based on their performance in a collection of games against each other.  We need data (the games and outcomes), and a model of how skill translates into these outcomes (e.g., higher skilled players have a better chance of winning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyGM as gm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple list of games & outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = [\n",
    "    (0,2, +1),  # P0 played P2 & won\n",
    "    (0,2, +1),  # played again, same outcome\n",
    "    (1,2, -1),  # P1 played P2 & lost\n",
    "    (0,1, -1),  # P0 played P1 and lost\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win probability and graphical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nplayers = max( [max(g[0],g[1]) for g in games] )+1\n",
    "nlevels = 10   # let's say 10 discrete skill levels\n",
    "scale = .3     # this scales how skill difference translates to win probability\n",
    "\n",
    "# Make variables for each player; value = skill level\n",
    "X = [None]*nplayers\n",
    "for i in range(nplayers):\n",
    "    X[i] = gm.Var(i, nlevels)   \n",
    "\n",
    "# Information from each game: what does Pi winning over Pj tell us?\n",
    "#    Win probability  Pr[win | Xi-Xj]  depends on skill difference of players\n",
    "Pwin = np.zeros( (nlevels,nlevels) )\n",
    "for i in range(nlevels):\n",
    "    for j in range(nlevels):\n",
    "        diff = i-j                   # find the advantage of Pi over Pj, then \n",
    "        Pwin[i,j] = (1./(1+np.exp(-scale*diff)))  # Pwin = logistic of advantage\n",
    "\n",
    "# before any games, uniform belief over skill levels for each player:\n",
    "factors = [ gm.Factor([X[i]],1./nlevels) for i in range(nplayers) ]\n",
    "\n",
    "# Now add the information from each game:\n",
    "for g in games:\n",
    "    P1,P2,win = g[0],g[1],g[2]\n",
    "    if P1>P2: P1,P2,win=P2,P1,-win  # (need to make player IDs sorted...)\n",
    "    factors.append(gm.Factor([X[P1],X[P2]], Pwin if win>0 else 1-Pwin) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gm.GraphModel(factors)\n",
    "model.makeMinimal()  # merge any duplicate factors (e.g., repeated games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: -2.471684774393813\n",
      "Iter 2: -3.006255512017563\n",
      "Iter 3: -3.1777481521503583\n",
      "Iter 4: -3.205567636371387\n",
      "Iter 5: -3.2108508120415307\n",
      "Iter 6: -3.2116774755212765\n",
      "Iter 7: -3.2118010272359205\n",
      "Iter 8: -3.211824198459521\n",
      "Iter 9: -3.211827816063664\n",
      "Iter 10: -3.21182835654903\n"
     ]
    }
   ],
   "source": [
    "if model.nvar < 0:       # for very small models, we can do brute force inference:\n",
    "    jt = model.joint()\n",
    "    jt /= jt.sum()       # normalize the distribution and marginalize the table\n",
    "    bel = [jt.marginal([i]) for i in range(nplayers)] \n",
    "else:                    # otherwise we need to use some approximate inference:\n",
    "    from pyGM.messagepass import LBP, NMF\n",
    "    lnZ,bel = LBP(model, maxIter=10, verbose=True)   # loopy BP\n",
    "    #lnZ,bel = NMF(model, maxIter=10, verbose=True)  # Mean field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization constant, $\\log(Z)$, represents the (log) probability of evidence for our model, namely the probability of the observed game outcomes given our parameters, etc.  We could experiment with changing the win probability function or its scaling parameter to try to make our model better fit the data using this value.\n",
    "\n",
    "For example, if you play with \"scale\" on these toy data, you'll find scale=0 (so that every game is a 50-50 chance independent of skill level) fits the data pretty well, because the few outcomes listed are not really consistent with skill determining outcome.  But, if you change the data so that there is an obvious ordering of skill (e.g., P0 then 1 and/or 2), a larger scale parameter will better fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ranking players by predicted skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean skill estimates: \n",
      "[5.3063527041093286, 4.5000000014256516, 3.6936472999475161]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean skill estimates: \")\n",
    "print([ bel[i].table.dot(np.arange(nlevels)) for i in range(nplayers)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting match outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability P0 beats P1 next time:\n",
      "0.547035108623\n"
     ]
    }
   ],
   "source": [
    "i,j = 0,1\n",
    "print(\"Estimated probability P{} beats P{} next time:\".format(i,j))\n",
    "# Expected value (over skill of P0, P1) of Pr[win | P0-P1]\n",
    "if i<j:\n",
    "    print( (bel[i]*bel[j]*gm.Factor([X[i],X[j]],Pwin)).table.sum() )\n",
    "else:\n",
    "    print( (bel[i]*bel[j]*gm.Factor([X[i],X[j]],1-Pwin)).table.sum() )\n",
    "    \n",
    "# Notes: we should probably use the joint belief over Xi and Xj, but for simplicity\n",
    "#  with approximate inference we'll just use the estimated singleton marginals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
